modules = ["python-3.11"]

[nix]
channel = "stable-25_05"
packages = ["bash", "glibcLocales", "libxcrypt", "ollama"]

[deployment]
deploymentTarget = "autoscale"
run = ["streamlit", "run", "app.py", "--server.port", "5000"]

[workflows]
runButton = "Project"

[[workflows.workflow]]
name = "Project"
mode = "parallel"
author = "agent"

[[workflows.workflow.tasks]]
task = "workflow.run"
args = "FastAPI Backend"

[[workflows.workflow.tasks]]
task = "workflow.run"
args = "Streamlit Frontend"

[[workflows.workflow.tasks]]
task = "workflow.run"
args = "Ollama Service"

[[workflows.workflow]]
name = "FastAPI Backend"
author = "agent"

[[workflows.workflow.tasks]]
task = "shell.exec"
args = "python -m uvicorn rag_backend:app --host 0.0.0.0 --port 8000"
waitForPort = 8000

[workflows.workflow.metadata]
outputType = "console"

[[workflows.workflow]]
name = "Streamlit Frontend"
author = "agent"

[[workflows.workflow.tasks]]
task = "shell.exec"
args = "streamlit run app.py --server.port 5000"
waitForPort = 5000

[workflows.workflow.metadata]
outputType = "webview"

[[workflows.workflow]]
name = "Ollama Service"
author = "agent"

[[workflows.workflow.tasks]]
task = "shell.exec"
args = "OLLAMA_HOST=0.0.0.0:11434 ollama serve"
waitForPort = 11434

[workflows.workflow.metadata]
outputType = "console"

[[ports]]
localPort = 5000
externalPort = 80

[[ports]]
localPort = 8000
externalPort = 8000

[[ports]]
localPort = 11434
externalPort = 3002

[[ports]]
localPort = 45689
externalPort = 3001

[[ports]]
localPort = 46865
externalPort = 3000

[agent]
expertMode = true
