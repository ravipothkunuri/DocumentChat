# RAG Application - Complete Implementation Prompt

## Project Overview
Build a production-ready Retrieval-Augmented Generation (RAG) application with FastAPI backend that allows users to upload documents, create embeddings, and query them using local LLMs via Ollama.

## Technical Stack
- **Backend**: FastAPI (Python)
- **Embeddings**: Ollama with nomic-embed-text model
- **LLM**: Ollama (llama3, mistral, phi, etc.)
- **Vector Storage**: Custom in-memory vector store with JSON persistence
- **Document Processing**: LangChain (PDF, TXT, DOCX support)
- **Similarity Search**: Cosine similarity with numpy/scikit-learn

## Core Requirements

### 1. Document Management
- **Upload Endpoint** (`POST /upload`)
  - Accept PDF, TXT, DOCX files via multipart/form-data
  - Validate file types before processing
  - Check for duplicate filenames
  - Return upload status with chunk count and file size
  - Handle errors gracefully with cleanup

- **Delete Endpoint** (`DELETE /documents/{filename}`)
  - Remove individual documents
  - Clean up metadata
  - Provide note about vector store rebuild

- **Clear All Endpoint** (`DELETE /clear`)
  - Remove all documents and embeddings
  - Clean up storage directories
  - Reset metadata

- **List Documents** (`GET /documents`)
  - Return all uploaded documents with metadata
  - Include: filename, size, chunks, status, upload date, type

- **Preview Document** (`GET /documents/{filename}/preview`)
  - Show first N chunks of a document
  - Return chunk IDs, content preview, and lengths

### 2. Document Processing Pipeline
- **Text Splitting**
  - Use RecursiveCharacterTextSplitter
  - Configurable chunk size (default: 1000)
  - Configurable chunk overlap (default: 200)
  - Add source and chunk_id metadata

- **Embedding Generation**
  - Use Ollama embeddings with nomic-embed-text
  - Generate embeddings for all chunks
  - Validate embedding dimensions
  - Prevent dimension mismatches

- **Vector Storage**
  - Store embeddings as numpy arrays (float32)
  - Maintain 1:1 mapping between documents and embeddings
  - Persist to JSON file
  - Load on startup

### 3. Query & Retrieval System
- **Query Endpoint** (`POST /query`)
  - Accept: question, model (optional), top_k, temperature
  - Generate query embedding
  - Perform similarity search
  - Retrieve top-k relevant chunks
  - Build context from retrieved chunks
  - Generate answer using LLM
  - Return: answer, sources, chunks_used, similarity_scores, processing_time

- **Similarity Search**
  - Use cosine similarity
  - Return top-k most similar documents
  - Include similarity scores in results
  - Handle empty vector store gracefully

### 4. Configuration Management
- **Configure Endpoint** (`POST /configure`)
  - Update LLM model
  - Update embedding model (with warning)
  - Update chunk size and overlap
  - Update temperature
  - Track changed fields
  - Warn about embedding model changes

- **Get Models** (`GET /models`)
  - List available Ollama models
  - Filter out embedding-only models
  - Show current LLM and embedding models
  - Handle ollama CLI failures gracefully

### 5. Health & Monitoring
- **Health Check** (`GET /health`)
  - Check Ollama connectivity
  - Test embedding generation
  - Return document count and query stats
  - Show configuration

- **Statistics** (`GET /stats`)
  - Total documents and chunks
  - Total queries processed
  - Total storage size
  - Average chunks per document
  - Last update timestamp

- **Debug Endpoints**
  - `GET /debug/embeddings`: Test embedding configuration
  - `GET /debug/vector-store`: Inspect vector store state
  - Show dimension consistency
  - Display sample documents

### 6. Error Handling & Recovery
- **Embedding Dimension Validation**
  - Validate dimensions when adding documents
  - Detect inconsistent dimensions on load
  - Auto-clear corrupted vector stores
  - Provide clear error messages

- **Rebuild Vectors** (`POST /rebuild-vectors`)
  - Reprocess all uploaded documents
  - Use current embedding model
  - Clear old embeddings
  - Maintain uploaded files
  - Report success/failure for each document

### 7. Logging & Debugging
- **Comprehensive Logging**
  - DEBUG: Detailed operation logs
  - INFO: Important events and results
  - WARNING: Non-critical issues
  - ERROR: Failures with stack traces
  - Include filename and line numbers

- **Log Key Operations**
  - API endpoint access
  - Document processing steps
  - Embedding generation
  - Query processing
  - Configuration changes
  - Errors and exceptions

## Data Models (Pydantic)

```python
class QueryRequest(BaseModel):
    question: str
    model: Optional[str]
    top_k: int = 4
    temperature: Optional[float]

class QueryResponse(BaseModel):
    answer: str
    sources: List[str]
    chunks_used: int
    similarity_scores: List[float]
    processing_time: float

class DocumentUploadResponse(BaseModel):
    status: str
    filename: str
    chunks: int
    file_size: int
    message: str

class ModelConfig(BaseModel):
    model: str
    embedding_model: Optional[str]
    chunk_size: Optional[int]
    chunk_overlap: Optional[int]
    temperature: Optional[float]

class DocumentInfo(BaseModel):
    filename: str
    size: int
    chunks: int
    status: str
    uploaded_at: str
    type: str
```

## Vector Store Implementation

### Key Features:
1. **In-Memory Storage**: Fast access with numpy arrays
2. **Persistence**: Save/load from JSON
3. **Dimension Validation**: Prevent mixed dimensions
4. **Efficient Search**: Vectorized cosine similarity
5. **Metadata Tracking**: Document sources and chunk IDs

### Methods:
- `add_documents(docs, embeddings)`: Add with validation
- `similarity_search(query_embedding, k)`: Find similar docs
- `get_documents_by_source(source)`: Filter by filename
- `save()`: Persist to disk
- `load()`: Load with validation
- `clear()`: Reset all data
- `get_stats()`: Return statistics

## Error Scenarios to Handle

1. **No Ollama Running**: Clear error message with setup instructions
2. **Model Not Available**: Suggest downloading model
3. **Dimension Mismatch**: Explain issue and provide rebuild option
4. **Duplicate Upload**: Reject with clear message
5. **Empty Vector Store**: Prevent queries, suggest upload
6. **File Processing Errors**: Clean up and report specific error
7. **Corrupted Vector Store**: Auto-detect and clear on load

## Performance Considerations

1. **Lazy Loading**: Initialize embeddings model on first use
2. **Batch Processing**: Process chunks in batches for embeddings
3. **Efficient Similarity**: Use numpy vectorization
4. **Memory Management**: Use float32 for embeddings
5. **Caching**: Cache embedding model instance

## Security & Validation

1. **File Type Validation**: Whitelist allowed extensions
2. **File Size Limits**: Prevent oversized uploads (via FastAPI)
3. **Input Sanitization**: Validate all user inputs with Pydantic
4. **Path Safety**: Use Path objects, prevent directory traversal
5. **CORS Configuration**: Restrict in production

## API Documentation

- Use FastAPI automatic documentation
- Provide clear endpoint descriptions
- Include request/response examples
- Document error responses
- Add tags for organization

## Directory Structure

```
project/
├── rag_app.py                 # Main application
├── uploaded_documents/        # Document storage
├── vector_data/              # Vector store persistence
│   ├── vectors.json          # Embeddings and documents
│   └── metadata.json         # Document metadata
└── requirements.txt          # Dependencies
```

## Installation & Setup

```bash
# Install dependencies
pip install fastapi uvicorn langchain langchain-community langchain-ollama 
pip install pypdf python-docx python-multipart ollama docx2txt 
pip install numpy scikit-learn

# Install Ollama
curl https://ollama.ai/install.sh | sh

# Pull required models
ollama pull llama3
ollama pull nomic-embed-text

# Run application
python rag_app.py
```

## Testing Checklist

- [ ] Upload PDF, TXT, DOCX files
- [ ] Query with different models
- [ ] Change configuration
- [ ] Delete individual documents
- [ ] Clear all documents
- [ ] Preview document chunks
- [ ] Check health endpoint
- [ ] View statistics
- [ ] Test debug endpoints
- [ ] Rebuild vectors after model change
- [ ] Handle dimension mismatch error
- [ ] Test with Ollama stopped
- [ ] Test duplicate uploads
- [ ] Test query on empty store
- [ ] Verify logging output

## Success Criteria

1. ✅ Successfully upload and process multiple document types
2. ✅ Generate accurate embeddings with consistent dimensions
3. ✅ Retrieve relevant document chunks for queries
4. ✅ Generate coherent answers using LLM
5. ✅ Handle all error scenarios gracefully
6. ✅ Provide comprehensive logging and debugging
7. ✅ Persist and restore vector store correctly
8. ✅ Validate dimension consistency automatically
9. ✅ Support multiple Ollama models
10. ✅ Provide clear API documentation

## Advanced Features (Optional)

1. **Streaming Responses**: Stream LLM output
2. **Batch Upload**: Multiple files at once
3. **Search Filters**: Filter by source, date, etc.
4. **Hybrid Search**: Combine semantic and keyword search
5. **Re-ranking**: Improve retrieval with cross-encoder
6. **Metadata Extraction**: Extract dates, authors, etc.
7. **Authentication**: Add user authentication
8. **Rate Limiting**: Prevent abuse
9. **Caching**: Cache query results
10. **Background Jobs**: Async document processing

## Deployment Considerations

1. **Docker**: Containerize application
2. **Environment Variables**: Externalize configuration
3. **Production Server**: Use gunicorn/uvicorn workers
4. **Reverse Proxy**: nginx for SSL and load balancing
5. **Monitoring**: Add metrics and alerts
6. **Backup**: Regular backup of vector store
7. **Scaling**: Consider vector database for large scale

---

## Quick Start Command

```bash
# After installation, run:
python rag_app.py

# Access API documentation:
http://localhost:8000/docs

# Test endpoints:
curl http://localhost:8000/health
```

## Key Implementation Notes

1. Always use `langchain_ollama` instead of deprecated imports
2. Filter embedding models from LLM dropdown
3. Validate embedding dimensions before adding to store
4. Provide clear error messages with actionable solutions
5. Log extensively for debugging
6. Handle Ollama connection failures gracefully
7. Clean up files on failed uploads
8. Use numpy float32 for memory efficiency
9. Implement proper CORS for frontend integration
10. Test with multiple embedding models to ensure compatibility